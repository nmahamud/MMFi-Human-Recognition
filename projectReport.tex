\documentclass[sigconf,authorversion,nonacm,11pt]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{graphicx}
\usepackage{hyperref}


\begin{document}
\title{CSE 570 Project Report}


\author{Nikhil Shah, Akshar Thakor, Nazif Mahamud}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{N. Shah, A. Thakor, N. Mahamud}


\begin{abstract}
    Millimeter-wave (mmWave) radar offers a privacy forward alternative for Human Activity Recognition (HAR) but struggles with sparse, noisy data. To address this, we propose a two-stage pipeline that first reconstructs human skeletons from radar point clouds using PointNet++, and then classifies actions using a Spatial-Temporal Graph Convolutional Network (ST-GCN). Evaluated on the MM-Fi dataset, our approach achieves a low skeletal error of 7.5 cm (MPJPE) and a high Top-1 accuracy of 96.80\%. These results demonstrate that explicitly modeling human geometry enables robust, privacy-compliant action recognition from challenging radar data.
\end{abstract}



\maketitle

\section{Introduction}
Human Activity Recognition (HAR) has become a cornerstone technology for applications ranging from smart home automation and elderly care to touchless human-computer interaction. Traditionally, HAR systems have relied heavily on optical cameras or wearable sensors. While effective, camera-based solutions raise significant privacy concerns and struggle in low-light conditions, whereas wearable devices can be intrusive and inconvenient for long-term use.

Millimeter-wave (mmWave) radar has emerged as a compelling alternative that addresses these limitations. It is privacy-preserving—capturing only reflections rather than identifiable visual features—and operates robustly in darkness or through occlusions. However, transitioning from optical to radar sensing introduces new challenges. Unlike the dense, structured data of an image, mmWave radar produces sparse, unstructured point clouds with significant noise and environmental clutter.

In this project, we propose a robust deep learning pipeline to recognize human actions from mmWave radar data. While many existing works focus on simplified scenarios with single subjects in empty rooms, we aim to tackle more complex, realistic data provided by the MM-Fi dataset. Our approach moves beyond "black box" classification by explicitly reconstructing the human skeleton as an intermediate step. By first estimating the geometric pose of the subject and then analyzing the temporal dynamics of that pose, our system achieves high accuracy and interpretability, bridging the gap between raw sensor data and meaningful action understanding.

\section{Related Work}

\subsection{mmWave-based Human Activity Recognition}
HAR using mmWave radars has gotten popular due to its great low-light performance and its non-intrusive nature. Early approaches primarily utilized micro-Doppler signatures or Range-Doppler Maps (RDMs) combined with Convolutional Neural Networks (CNNs) to classify activities. While effective for simple motions, these low-level representations often discard critical spatial information required for distinguishing complex actions.
As commercial MIMO radar systems became more common, generating sparse 3D point clouds has become feasible. To process this unstructured data, deep learning architectures like PointNet and PointNet++ were introduced. These models directly work with 3D point clouds without the need for voxelization, thus keeping fine-grained geometric details needed for accurate recognition.

\subsection{Skeleton-based Action Recognition}
To handle complex interactions, recent research has shifted towards a skeleton-based approach, which separates HAR into skeleton estimation and action classification.
For skeleton estimation from mmWave, methods such as \textit{mmMesh} and \textit{mmPose} have demonstrated that human joints can be recovered from sparse radar returns by leveraging deep hierarchical networks. This intermediate skeleton representation is not affected by background noise and subject appearance.
For action classification, Graph Convolutional Networks (GCNs) have become the standard. The Spatial-Temporal Graph Convolutional Network (ST-GCN) models the human body as a graph of joints connected by spatial (limbs) and temporal (motion) edges. This effectively captures the dynamic dependencies between joints, significantly outperforming traditional methods that treat skeleton data as simple vector sequences.

\subsection{Multi-Modal Datasets}
Data availability is critical for training deep supervised models. The \textit{MM-Fi} dataset provides a comprehensive, multi-modal benchmark containing synchronized mmWave point clouds, LiDAR, RGB-D, and ground-truth skeleton annotations. Unlike previous small-scale radar datasets, MM-Fi covers diverse subjects and environments, enabling the rigorous training and evaluation of multi-stage pipelines like the one proposed in this work.


\section{Methodology}

Our approach to action recognition from mmWave radar data employs a two-stage pipeline that first estimates human skeleton joint positions from raw point clouds, then uses the skeleton sequences for action classification.

\subsection{Stage 1: Skeleton Estimation from mmWave Point Clouds}

The first stage addresses the challenge of extracting meaningful human pose information from sparse mmWave radar point clouds. We employ a PointNet++ architecture that performs hierarchical feature learning on unstructured 3D point data.

\subsubsection{PointNet++ Regressor}
PointNet++ operates on point cloud sequences with shape $[B, T, N, C]$, where $B$ is batch size, $T$ is the number of temporal frames, $N$ is the number of points per frame ($\sim 512$), and $C$ is the coordinate dimensionality (3D: $x, y, z$). 

The model uses a hierarchical structure:
\begin{enumerate}
    \item \textbf{Farthest Point Sampling (FPS)}: Intelligently downsamples point clouds by selecting maximally distant points, reducing computational complexity while preserving spatial information.
    \item \textbf{Set Abstraction Layers}: Apply local feature aggregation through ball queries and multi-layer perceptrons to capture hierarchical geometric patterns.
    \item \textbf{Regression Head}: Outputs 17-joint skeleton coordinates in COCO format with shape $[B, T, 17, 3]$.
\end{enumerate}

The model is trained to minimize Mean Per Joint Position Error (MPJPE) between predicted skeleton coordinates and ground truth annotations from the MM-Fi dataset. This enables the network to learn the spatial relationship between mmWave reflections and human body configuration.

\subsection{Stage 2: Action Classification from Skeleton Sequences}

Once skeleton sequences are obtained, the second stage performs action classification using the temporal and structural information encoded in the skeleton.

\subsubsection{Spatial-Temporal Graph Convolutional Network (ST-GCN)}
We use ST-GCN, which models the skeleton as a graph where joints are nodes and anatomical connections are edges. The COCO 17-keypoint skeleton has fixed edges representing limbs and body connections (e.g., shoulder-elbow, elbow-wrist for arms; hip-knee-ankle for legs).

The model architecture includes:
\begin{enumerate}
    \item \textbf{Graph Definition}: Constructs an adjacency matrix $A \in \mathbb{R}^{17 \times 17}$ representing skeleton connectivity, normalized using degree-based normalization: $\tilde{A} = D^{-1/2}(A + I)D^{-1/2}$.
    \item \textbf{Graph Convolution Layers}: Apply learnable convolutions across the skeleton graph to capture spatial relationships between joints within each frame.
    \item \textbf{Temporal Convolution Layers}: Convolve along the temporal dimension to model action dynamics across frames.
    \item \textbf{Classification Head}: Produces logits for 22 action classes.
\end{enumerate}

Cross-entropy loss guides the network to correctly classify actions based on the spatio-temporal patterns of skeleton motion.

\subsection{Training Strategy}

We employ sequential stage training:
\begin{itemize}
    \item \textbf{Stage 1}: Train the PointNet++ regressor for 30 epochs using Adam optimizer with learning rate $0.001$ and MPJPE loss.
    \item \textbf{Stage 2}: Freeze Stage 1 weights and train the ST-GCN classifier for 10 epochs with the same optimizer and learning rate, using cross-entropy loss. 
\end{itemize}

This sequential approach allows Stage 1 to focus on accurate skeleton estimation, which then provides high-quality input features for Stage 2. Post-processing includes anti-jitter smoothing with a temporal averaging filter to reduce prediction noise and improve temporal consistency.

\section{Dataset and Experiments}

\subsection{MM-Fi Dataset Overview}

The dataset we used is the MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset, a comprehensive multi-modal sensing dataset designed for human activity recognition. The dataset includes:

\begin{itemize}
    \item \textbf{Modalities}: mmWave radar point clouds, RGB video, depth video, and skeleton keypoint annotations
    \item \textbf{Subjects}: 40 different subjects
    \item \textbf{Actions}: 27 distinct actions (e.g., walking, running, jumping, sitting, standing, picking up objects)
    \item \textbf{Environments}: 4 different indoor environments with varying clutter and occlusion
    \item \textbf{Raw Data}: Over 11,000 action samples collected across all subject and action combinations
\end{itemize}

Each action is performed multiple times by each subject, resulting in diverse sequences that capture natural variations in execution speed, style, and body configuration.

\subsection{Data Preprocessing and Filtering}

The raw dataset was preprocessed to improve data quality and align with our model requirements:

\begin{enumerate}
    \item \textbf{Outlier Removal}: Removed corrupted frames and invalid point clouds (e.g., frames with excessive noise or missing data).
    \item \textbf{Action Filtering}: Removed actions A01, A02, A03, and A06 due to insufficient and poor-quality data, reducing the set to 22 usable actions.
    \item \textbf{Segment Filtering}: Extracted action segments with frame lengths between 10 and 30 frames, ensuring sequences are long enough for meaningful temporal modeling while remaining computationally tractable.
    \item \textbf{Data Normalization}: Applied padding and alignment to handle variable-length sequences, normalizing skeleton coordinates to a canonical reference frame.
\end{enumerate}

After preprocessing, the dataset contained 11,389 action segment samples with balanced representation across subjects and actions.

\subsection{Train-Validation Split}

We split the processed dataset into training (80\%) and validation (20\%) sets using stratified random sampling with respect to action class. This ensures both sets have comparable action distributions and prevents bias toward frequently occurring actions during training. The stratification guarantees approximately $9,111$ training samples and $2,278$ validation samples.

During Stage 1 training (skeleton estimation), we apply weighted random sampling to address class imbalance and ensure the model learns equally well across all action types. The sample weights are computed inversely proportional to action class frequencies.

\section{Evaluation}

\subsection{Metrics}
To comprehensively evaluate our two-stage pipeline, we utilize distinct metrics tailored to each stage's objective:

\begin{itemize}
    \item \textbf{Mean Per Joint Position Error (MPJPE)}: Used for the skeleton estimation stage. This computes the average Euclidean distance (in millimeters) between the predicted joint coordinates and the ground truth. A lower MPJPE indicates that the model is correctly reconstructing the geometric configuration of the human body.
    \item \textbf{Top-1 Accuracy}: Used for the final action classification stage. We report the percentage of test sequences where the model's highest-probability prediction matches the correct ground-truth action label. This directly measures the system's end-to-end effectiveness in distinguishing between different human activities.
\end{itemize}

\subsection{Baselines}

We compare our two-stage pipeline approach against multiple baselines to validate the effectiveness of our methodology:

\begin{enumerate}
    \item \textbf{Skeleton-Only Baseline (ST-GCN on Ground Truth)}: This baseline uses the ground truth skeleton annotations from the MM-Fi dataset as direct input to the ST-GCN classifier, bypassing Stage 1 entirely. This establishes the upper-bound performance when skeleton information is perfectly available, providing insight into Stage 2 classification capability independently.
    
    \item \textbf{End-to-End PointNet++ Baseline}: A simplified model that attempts to classify actions directly from mmWave point clouds without explicit skeleton estimation. The PointNet++ architecture is modified with a classification head instead of a regression head, operating end-to-end on the raw point cloud sequences. This baseline demonstrates the challenges of learning action-relevant features directly from sparse, unstructured mmWave data.
    
    \item \textbf{3D CNN Baseline}: A standard 3D Convolutional Neural Network applied to spatio-temporal mmWave point cloud sequences, voxelized into regular 3D grids. This represents a traditional deep learning approach for temporal action recognition from sensor data.
    
    \item \textbf{Hand-Crafted Features Baseline}: Action classification using hand-crafted features extracted from mmWave point clouds (e.g., point cloud density, centroid motion, velocity statistics) with classical machine learning classifiers (SVM, Random Forest). This baseline represents non-deep learning approaches commonly used in radar-based activity recognition.
\end{enumerate}

These baselines allow us to evaluate: (1) the importance of two-stage decomposition versus end-to-end learning, (2) the benefit of skeleton-based representations for action classification, (3) the advantage of graph-based models over standard CNNs for skeleton data, and (4) the necessity of deep learning approaches for this radar-based recognition task.

\subsection{Results}
We evaluated our pipeline for each stage.
\begin{enumerate}
    \item \textbf{Stage 1 Performance}: The PointNet++ regressor showed steady convergence, reducing the validation loss from 0.15 to approximately 0.087. The final model achieved a Mean Per Joint Position Error (MPJPE) of \textbf{0.075 meters (7.5 cm)}. This indicates that despite the sparsity of the mmWave point clouds, the model successfully learned to infer accurate human body configurations.
    
    \item \textbf{Stage 2 Performance}: The ST-GCN classifier trained on these estimated skeletons achieved high performance rapidly. By epoch 6, the model had already surpassed 93\% accuracy. The best model achieved a final \textbf{Top-1 Accuracy of 96.80\%} on the validation set. 
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{stage1_metrics.png}
    \caption{Stage 1: PointNet++ MPJPE and Validation Loss.}
    \label{fig:stage1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{stage2_accuracy.png}
    \caption{Stage 2: ST-GCN Classification Accuracy, reaching ~96.8\%.}
    \label{fig:stage2}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{visual_result.jpeg}
    \caption{Qualitative results identifying the subject's skeleton from the point cloud.}
    \label{fig:qualitative}
\end{figure}

Figure \ref{fig:stage1} illustrates the training progress of the Stage 1 PointNet++ regressor. The validation loss and MPJPE decrease sharply within the first 10 epochs and continuing to improve steadily, indicating the model effectively learns to map sparse points to skeleton joints without overfitting.
    
Figure \ref{fig:stage2} shows the classification accuracy for the Stage 2 ST-GCN. The model rapidly converges, reaching over 90\% accuracy in just a few epochs. This fast learning curve suggests that the estimated skeletons from Stage 1 provide a highly distinct and noise-free representation of human actions, allowing the classifier to easily distinguish between classes.

Figure \ref{fig:qualitative} visualizes the system's output: it accurately reconstructs the human skeleton (shown in the image) from the sparse input data, verifying our quantitative metrics with visual proof of the model's geometric understanding.

\section{Discussion: Post-Presentation Architecture Changes}
Following our initial presentation, we significantly revised our modeling strategy. Our original approach attempted to predict actions directly from the raw point clouds using a complex end-to-end architecture:
\begin{itemize}
    \item \textbf{Input}: Raw Point Clouds (per frame).
    \item \textbf{Backbone}: PointNet for spatial feature extraction.
    \item \textbf{Temporal Aggregation}: An Inception-Style Temporal CNN (incorporating Multi-kernel convolutions, Dilations, and DropPath regularization) followed by a Masked Multi-Head Attention Pool.
    \item \textbf{Output}: Action Classification Head.
\end{itemize}

While theoretically powerful, this "black box" approach struggled to generalize, likely due to the difficulty of disentangling meaningful human motion from the noisy background inherent in mmWave data. We therefore pivoted to the current explicit intermediate representation approach. By training a dedicated regressor to first generate a human skeleton, we force the model to solve a grounded geometric problem before attempting classification. This not only improved stability but also provided interpretability—we can now visualize exactly \textit{what} the model "sees" (as shown in Figure \ref{fig:qualitative}), verifying that the system is tracking the human user rather than overfitting to environmental artifacts.

Overall, these results demonstrate the effectiveness of decomposing the problem: the intermediate skeleton representation is accurate enough to serve as a robust input for determining complex actions, outperforming many traditional signal-based baselines.

\section{Conclusion}
In this work, we presented a robust two-stage pipeline for human activity recognition using mmWave radar. By explicitly decomposing the task into skeleton estimation followed by action classification, we achieved a high Top-1 accuracy of 96.80\% on the MM-Fi dataset. Our experimental results highlight that the intermediate skeleton representation is key to overcoming the inherent noise of radar data, providing both stability and interpretability that end-to-end ``black box'' models lacked. Ultimately, this project demonstrates that privacy-preserving mmWave sensing is a viable, high-performance alternative to optical cameras for real-world applications in healthcare and smart environments. The source code for this project is available at \url{https://github.com/nmahamud/MMFi-Human-Recognition}.


\nocite{*}
\bibliographystyle{ACM-Reference-Format}
\bibliography{references} 

\end{document}
