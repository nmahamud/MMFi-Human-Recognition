\documentclass[sigconf,authorversion,nonacm,11pt]{acmart}

\usepackage{booktabs} % For formal tables


\begin{document}
\title{CSE 570 Project Report}




\author{Nikhil Shah, Akshar Thakor, Nazif Mahamud}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{N. Shah, A. Thakor, N. Mahamud}


\begin{abstract}
    This is a project that uses mmWave radar for action recognition of humans. 
\end{abstract}



\maketitle

\section{Introduction}
Identifying actions that humans perform is always a challenging thing a computer has to do. But it enables us to use the computer in different ways, often without even touching anything. 
Existing works have focused on identifying actions in a completely empty room with only a single person in view. With our project, we should be closer to recognizing the actions even though the radar is picking up other information.

\section{Related Work}

What is/are the current state of the art and what are the limitations to current approaches?


\section{Methodology}

Our approach to action recognition from mmWave radar data employs a two-stage pipeline that first estimates human skeleton joint positions from raw point clouds, then uses the skeleton sequences for action classification.

\subsection{Stage 1: Skeleton Estimation from mmWave Point Clouds}

The first stage addresses the challenge of extracting meaningful human pose information from sparse mmWave radar point clouds. We employ a PointNet++ architecture that performs hierarchical feature learning on unstructured 3D point data.

\subsubsection{PointNet++ Regressor}
PointNet++ operates on point cloud sequences with shape $[B, T, N, C]$, where $B$ is batch size, $T$ is the number of temporal frames, $N$ is the number of points per frame ($\sim 512$), and $C$ is the coordinate dimensionality (3D: $x, y, z$). 

The model uses a hierarchical structure:
\begin{enumerate}
    \item \textbf{Farthest Point Sampling (FPS)}: Intelligently downsamples point clouds by selecting maximally distant points, reducing computational complexity while preserving spatial information.
    \item \textbf{Set Abstraction Layers}: Apply local feature aggregation through ball queries and multi-layer perceptrons to capture hierarchical geometric patterns.
    \item \textbf{Regression Head}: Outputs 17-joint skeleton coordinates in COCO format with shape $[B, T, 17, 3]$.
\end{enumerate}

The model is trained to minimize Mean Per Joint Position Error (MPJPE) between predicted skeleton coordinates and ground truth annotations from the MM-Fi dataset. This enables the network to learn the spatial relationship between mmWave reflections and human body configuration.

\subsection{Stage 2: Action Classification from Skeleton Sequences}

Once skeleton sequences are obtained, the second stage performs action classification using the temporal and structural information encoded in the skeleton.

\subsubsection{Spatial-Temporal Graph Convolutional Network (ST-GCN)}
We use ST-GCN, which models the skeleton as a graph where joints are nodes and anatomical connections are edges. The COCO 17-keypoint skeleton has fixed edges representing limbs and body connections (e.g., shoulder-elbow, elbow-wrist for arms; hip-knee-ankle for legs).

The model architecture includes:
\begin{enumerate}
    \item \textbf{Graph Definition}: Constructs an adjacency matrix $A \in \mathbb{R}^{17 \times 17}$ representing skeleton connectivity, normalized using degree-based normalization: $\tilde{A} = D^{-1/2}(A + I)D^{-1/2}$.
    \item \textbf{Graph Convolution Layers}: Apply learnable convolutions across the skeleton graph to capture spatial relationships between joints within each frame.
    \item \textbf{Temporal Convolution Layers}: Convolve along the temporal dimension to model action dynamics across frames.
    \item \textbf{Classification Head}: Produces logits for 22 action classes.
\end{enumerate}

Cross-entropy loss guides the network to correctly classify actions based on the spatio-temporal patterns of skeleton motion.

\subsection{Training Strategy}

We employ sequential stage training:
\begin{itemize}
    \item \textbf{Stage 1}: Train the PointNet++ regressor for 30 epochs using Adam optimizer with learning rate $0.001$ and MPJPE loss.
    \item \textbf{Stage 2}: Freeze Stage 1 weights and train the ST-GCN classifier for 100 epochs with the same optimizer and learning rate, using cross-entropy loss.
\end{itemize}

This sequential approach allows Stage 1 to focus on accurate skeleton estimation, which then provides high-quality input features for Stage 2. Post-processing includes anti-jitter smoothing with a temporal averaging filter to reduce prediction noise and improve temporal consistency.

\section{Dataset and Experiments}

\subsection{MM-Fi Dataset Overview}

The dataset we used is the MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset, a comprehensive multi-modal sensing dataset designed for human activity recognition. The dataset includes:

\begin{itemize}
    \item \textbf{Modalities}: mmWave radar point clouds, RGB video, depth video, and skeleton keypoint annotations
    \item \textbf{Subjects}: 40 different subjects
    \item \textbf{Actions}: 27 distinct actions (e.g., walking, running, jumping, sitting, standing, picking up objects)
    \item \textbf{Environments}: 4 different indoor environments with varying clutter and occlusion
    \item \textbf{Raw Data}: Over 11,000 action samples collected across all subject and action combinations
\end{itemize}

Each action is performed multiple times by each subject, resulting in diverse sequences that capture natural variations in execution speed, style, and body configuration.

\subsection{Data Preprocessing and Filtering}

The raw dataset was preprocessed to improve data quality and align with our model requirements:

\begin{enumerate}
    \item \textbf{Outlier Removal}: Removed corrupted frames and invalid point clouds (e.g., frames with excessive noise or missing data).
    \item \textbf{Action Filtering}: Removed actions A01, A02, A03, and A06 due to insufficient and poor-quality data, reducing the set to 22 usable actions.
    \item \textbf{Segment Filtering}: Extracted action segments with frame lengths between 10 and 30 frames, ensuring sequences are long enough for meaningful temporal modeling while remaining computationally tractable.
    \item \textbf{Data Normalization}: Applied padding and alignment to handle variable-length sequences, normalizing skeleton coordinates to a canonical reference frame.
\end{enumerate}

After preprocessing, the dataset contained 11,389 action segment samples with balanced representation across subjects and actions.

\subsection{Train-Validation Split}

We split the processed dataset into training (80\%) and validation (20\%) sets using stratified random sampling with respect to action class. This ensures both sets have comparable action distributions and prevents bias toward frequently occurring actions during training. The stratification guarantees approximately $9,111$ training samples and $2,278$ validation samples.

During Stage 1 training (skeleton estimation), we apply weighted random sampling to address class imbalance and ensure the model learns equally well across all action types. The sample weights are computed inversely proportional to action class frequencies.

\section{Evaluation}

\subsection{Metrics}
What are the metrics being used to evaluate your system and why? Examples: True Positive Rate, Accuracy, F-1 Score, Balanced Accuracy, etc.

\subsection{Baselines}
What are you comparing your system with? A baseline can be a simple straw solution or an existing approach.

\subsection{Results}
This is the most important part of the report. Please include graphs to present your results and explain them.

\section{Conclusion}
Describe the impact your project will have within the field, community, and wider audience.


\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-bibliography} 

\end{document}
